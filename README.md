# MDEval

A benchmark to evaluate the _Markdown Awareness_ of LLMs chatbot's outputs.

## Human Evaluation Arena

To verify the effectiveness of the proposed benchmark, we built a human evaluation arena where we asked human annotators to evaluate the outputs of the LLMs chatbots. The arena is available at [this link](https://md-eval-human.pages.dev/).


It is recommended to use `virtualenv` to create a virtual environment, and then install the dependencies using `pip install -r requirements.txt`.

Before using MDEval, you need to set your key to the environment variables first.